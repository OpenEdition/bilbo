
----------------------------------------------
 April 2 2012, by Young-Min KIM					
 Short description of prototype codes			
 Current : sequential processing of functions	
 Goal : OOP, use of class, modularization		
----------------------------------------------


==============================================
 formatting
==============================================

------------------------
 dataGenerator.py
------------------------
 
 def processing (fname)
 
	From a file, read lines then make an instance of BeautifulSoup.
	- elimination (tmp_str) : string cleaning
	- loop for <bibl>
		- extract all tags
		- iterate per content (in BeautifulSoup, separated by tags, unwrapped string becomes a content)
			if content has no tags, provide 'nolabel'
			if has, call extract_tags(c_tag) 
		- a problem for applying BeautifulSoup : another <bibl> in a <bibl> is not correctly captured
			inner <bibl> is always after <relateditem> <- indicator
			so artificially count the iterator, then re-process as above.


 def extract_tags(current_tag)
 
 	From current string including tags and contents, extract tokens, labels and attributes.
 	- cleaning of string, elimination of enter character, meaningless whitespace etc.
 	- arrangeData(n, txts, tags, attrs, top_tag, top_att) : extract data in a tree structure
 		

 def arrangeData(n, txts, tags, attrs, top_tag, top_att)

	Flatten the tree.
	As input, 	n:currnet_tag string, txts: txt string, tags:including tags, attrs:including attrs,
				top_tag: can be attached to current string without other tag
				top_att: can be attached to current string
				for the child elements, top_tag and top_att are continuously added.


 def elimination (tmp_str)
 
 	elimination of the obstructive <h> tags
 
				
 *comments* 
 Think about replacing BeautifulSoup as ElementTree.
 Try to consider a more generalized case for the elimination.
 
	 
 	
 	
------------------------ 
 puncOrganizer.py
------------------------

 def reorganizing(fname)
 	
 	Read line by line the result obtained from dataGenerator.py
 	When the input token is valid, continue to process.
 	
 	- It allows the extraction of basic features 
 	
 	#posseditor check
 	retrn_str = editorCheck(input_str)
 	#initial check
 	retrn_str = initCheck(input_str)
 	#posspage check
 	retrn_str = pageCheck(input_str)
 	#http, www check
 	retrn_str = refCheck(input_str)
 	
 	#tokenization
 	Separation of tokens, consideration of special characters (French characters): special.has_key(new_str)
 	When at least a punctuation mark is in the string, separate them. 
 		Printing line directly (to be modified)
 	
 	When punctuation marks are not detected (or one of the above feature check is valid), add features.
 	Special character check is also included
 		Printing line directly (to be modified)
 		
 	- During the above processing, basic feature extraction: feat_str = featureCheck(new_str)
 	
 *comments* 	
 Pure sequential line processing. If we want an OOP, just need to move these functions in the classes.	
 	
 	
 	
------------------------ 
 extractor.py
------------------------

 def extNumDocs (filename)
 	Extract the number of documents separated with blocks
 	
 
 def randomgen(numb)	
 	Generate the indicators for the training and test documents	
 	
 
 def extractor (filename, ndocs, tr, extr) 
 
 	Extract the training and test data : now read a file
 	Now separated for the training, test, and new data : need to integrate them
 	- tr indicator check, it gives the valid instance indices 
 	- extraction type check, if it is for new data, if we need rich data etc.
 		- too separated, need to arrange
 		
 	- relatedItem check, to differentiate title types
 	- the closest tag is selected as label of token
 		- there are exceptions, label in {nonLabels} is replaced by its parent tag
 		- if there are no valid labels till top, give <nolabel>
 			- but there exist <abbr>, <ref> take them as label.
 			!! too detailed exceptions.... if we want to modify them ? need an official channel
 	- <nonbibl> is special, if a token has <nonbibl>, regardless of other tags, its label is <nonbibl>
 	- title treatment is complicated cause we consider both type of annotated title and its location
 		<title> vs. <booktitle>
 	- incorporation of some similar tags
 	
 	- attribute extraction
 	- <bookindicator>
 	- we can add attributes <- to be considered as a function, now #punctuation check
 	- small functions to treat exceptions <- to be rearranged
 		- def extract_attrs(attstr) : input is line, need to extract attributes <- to be modified 
 		- def check_att(attstr, attrname) : see if the attribute is in the string <- would be useful
 
 	- according to tr, different printing <- to be modified 
 	- proper noun lists check, <- to be detached as a function
 	- printing
 	
 	
def printdata(All_bibls)
	final printing


def addlayout(All_bibls)
	layout features


def printmorefeatures(All_bibls, extr) 
	extract rich features for SVM note classification


def extract_attrs(attstr)
	tokenize attributes	


def check_att(attstr, attrname)
	see if the attribute is in the string


def extract_title(attstr, relatItm, titleCK, titleAttr)
	title tag arrangement


------------------------ 
 nameProcess.py
------------------------

To ch

chercher les nom propre dans une liste et si il sont prŽsent il ajoute un feature surname list





------------------------ 
 preparerCRF.py
------------------------

 Call the codes to prepare learning/test data for CRF
 Problem : input and output are discontinuous, execute sequentially the python codes
 			<- to be handled
 			

------------------------ 
 repreparerCRF.py
------------------------

 For the new data extraction, need to be integrated into the above code
 
 
 
 
 
==============================================
 Machine Learning
==============================================





















