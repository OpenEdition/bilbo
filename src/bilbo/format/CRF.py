# -*- coding: utf-8 -*-
from __future__ import unicode_literals
"""
Created on April 18, 2012

@author: Young-Min Kim, Jade Tavernier
"""
from bilbo.format.Extract_crf import Extract_crf
from bilbo.reference.ListReferences import ListReferences 
from bilbo.output.GenerateXml import GenerateXml
import re
import Stemmer
import subprocess, os
from codecs import open

class CRF(object):
	"""
	CRF object is created in a Bilbo object
	CRF model learning and test
	"""

	def __init__(self, dirResult, options={}):
		"""
		Attributes
		----------
		generateXml : GenerateXml
		dirResult : string
			directory for result files
		"""
		self.generateXml = GenerateXml()
		self.dirResult = dirResult
		self.options = options
		self.dirModel = ""
		main = os.path.realpath(__file__).split('/')
		self.rootDir = "/".join(main[:len(main)-4])


	def setDirModel(self, dirModel):
		self.dirModel = dirModel


	def prepareTrain(self, corpus, typeCorpus, fileRes, tr=-1, extOption=-1, optsvm=True):
		"""
		Prepare CRF training data
		
		Parameters
		----------
		corpus : Corpus
		typeCorpus : int, {1, 2, 3}
			1 : corpus 1, 2 : corpus 2...
		fileRes : string
			output file name
		tr : int, {1, 0, -1, -2} (default -1)
			check if training or test data
			1 : train, 0 : test without label, -1 : test with label, -2 : test only label
		extOption : int, {-1, 1, ...} (default -1)
			extra option for crf training/test data format
			check if data is internal data, if yes we'll use a modified index for corpus type 2
			-1 : data format for SVM
			1 : data format for normal CRF training/test data
			2-5 : (not yet provided)
		"""
		listReferences = corpus.getListReferences(typeCorpus)
		newListReferences = ListReferences(listReferences, typeCorpus)
		extractor = Extract_crf(self.options)
		nbRef = corpus.nbReference(typeCorpus)

		'generation of training index for each reference'
		extractor.randomgen(newListReferences, 1)
				
		'if corpus type 2 and extOption=1, we use a modified index list' #!!!!!!!!!!
		if typeCorpus == 2 and extOption == 1:
			'modify the indices to eliminate the reference (or not print the reference) classified as non-bibl BY SVM'
			if optsvm == True : #if not, do not modify
				extractor.extractIndices(self.dirResult+"svm_predictions_training", newListReferences)
			extractor.extract(typeCorpus, nbRef, self.dirResult+fileRes, newListReferences, tr, extOption)
			
		else: # typeCorpus == 1 or (typeCorpus == 2 and isFrstExt == -1)
			########## SOURCE DATA EXTRACTION FOR SVM OR CORPUS 1 (BUT THESE ARE DIFFERENT !!!)
			extractor.extract(typeCorpus, nbRef, self.dirResult+fileRes, newListReferences, tr, extOption)
		
		return


	def prepareTest(self, corpus, typeCorpus, indiceSvm = 0):
		"""
		Prepare CRF test data
		
		Parameters
		----------
		corpus : Corpus
		typeCorpus : int, {1, 2, 3}
			1 : corpus 1, 2 : corpus 2...
		indiceSvm : int, {0, -1, 2}
			0 : normal(corpus 1)
			-1 : data04SVM (corpus2),
			2 : external data => svm isn't called
		"""
		listReferences = corpus.getListReferences(typeCorpus)
		listReferencesObj = ListReferences(listReferences, typeCorpus)
		
		extractor = Extract_crf(self.options)
		nbRef = corpus.nbReference(typeCorpus)
		
		'generation of test index for each reference'
		extractor.randomgen(ListReferences(listReferencesObj.getReferences(),typeCorpus), 0)
		
		if indiceSvm == -1:
			extractor.extract(typeCorpus, nbRef, self.dirResult+"data04SVM_ori.txt", ListReferences(listReferencesObj.getReferences(),typeCorpus))
		else:
			'file for CRF training'
			if typeCorpus == 2 and indiceSvm != 2 :
				extractor.extractIndices4new(self.dirResult+"svm_predictions_new", ListReferences(listReferencesObj.getReferences(),typeCorpus))
			
			extractor.extract(typeCorpus, nbRef, self.dirResult+"testdatawithlabel_CRF.txt",ListReferences(listReferencesObj.getReferences(),typeCorpus), -1, 1)
			extractor.extract(typeCorpus, nbRef, self.dirResult+"testdata_CRF.txt",ListReferences(listReferencesObj.getReferences(),typeCorpus), 0, 1)

		return ListReferences(listReferencesObj.getReferences(),typeCorpus)


	def runTrain(self, directory, fichier, modelname, penalty=0.00001) :
		"""
		Run CRF training module from Wapiti software
		
		Parameters
		----------
		directory : string
			directory where we save the model
		fichier : string
			filename that has been generated by preprareTrain
		"""
		dependencyDir = os.path.join(self.rootDir, 'dependencies')
		command = dependencyDir+"/wapiti-1.4.0/wapiti train -p "+self.rootDir+"/KB/config/wapiti/pattern_ref -2 "+str(penalty)+" "+self.dirResult+fichier+" "+directory+modelname
		process = subprocess.Popen(command, shell=True, stdout=subprocess.PIPE)
		process.wait()

		return


	def runTest(self, directory, fichier, modelname, addStr="") :
		"""
		Run CRF test module from Wapiti software to label new data
		
		Parameters
		----------
		directory : string
			directory where we save the model
		fichier : string
			filename that has been generated by preprareTest
		"""
		dependencyDir = os.path.join(self.rootDir, 'dependencies')
		command = dependencyDir+"/wapiti-1.4.0/wapiti label -m "+directory+modelname+" "+self.dirResult+fichier+" "+self.dirResult+"testEstCRF"+addStr+"_Wapiti.txt"
		process = subprocess.Popen(command, shell=True, stdout=subprocess.PIPE)
		process.wait()
	
		'Create testEstCRF.txt keeping only predicted labels'
		fafter = open(self.dirResult+"testEstCRF"+addStr+".txt", 'w', encoding='utf8')
		for line in open(self.dirResult+"testEstCRF"+addStr+"_Wapiti.txt", 'r', encoding='utf8') :
			line = line.split()
			if len(line) > 0 :
				fafter.write(str(line[len(line)-1]))
				fafter.write("\n")
			else : fafter.write("\n")
		fafter.close()
		if addStr == "" :
			self.generateXml.simpleComp(self.dirResult+"testdata_CRF.txt", self.dirResult+'testEstCRF.txt', 2, self.dirResult+'testEstCRF.xml')
		return


	def postProcessTest(self, fnameCRFresult, fnameCRFtoAdd, refsAfterSVM):
		"""
		Post-processing of labeling result. After a normal CRF labeling, we return to the SVM classification result,
		then check which notes should be annotated as non-bibliographic ones, then actually modify the labeling result.
		
		Parameters
		----------
		fnameCRFresult : string
			directory where we save the model
		fnameCRFtoAdd : string
			filename that has been generated by preprareTest
		refsAfterSVM : list of 'Reference' objects
			reference list containing SVM classification result
		"""
		
		fbefore = open(self.dirResult+fnameCRFresult, 'r')
		fafter = open(self.dirResult+fnameCRFtoAdd, 'w')
		
		for reference in refsAfterSVM :
			if reference.train != -1 :
				line = fbefore.readline()
				while (len(line.split()) > 0) :
					fafter.write(str(line))
					line = fbefore.readline()
				fafter.write("\n")
			elif len(reference.getWord()) > 0 : # if there is no word in the reference, it was already ignored in printing before
				line = fbefore.readline()
				while (len(line.split()) > 0) :
					fafter.write("nonbibl \n")
					line = fbefore.readline()
				fafter.write("\n")
		fafter.close()
		fbefore.close()
		
		self.generateXml.simpleComp(self.dirResult+"testdata_CRF.txt", self.dirResult+fnameCRFtoAdd, 2, self.dirResult+'testEstCRF.xml')
		
		return

		''' FONCTION TREE TAGGER '''

	def _prepareAnalyse(self, corpus, typeCorpus, fileRes):

		"""

		Retrieve token for Tree Tagger Analyse

		"""
		dependencyDir = os.path.join(self.rootDir, 'dependencies')
		fileResult = open(os.path.join(self.dirResult, 'treeTaggerAnalyse.txt'), 'w', encoding='utf-8')

		listRef = corpus.getListReferences(typeCorpus)
		newListReferences = ListReferences(listRef, typeCorpus)
		listReferences = newListReferences.getReferences()

		for reference in listReferences:
			for mot in reference.getWord():
				fileResult.write(mot.nom)
				fileResult.write('\n')
		
		fileResult.close()
		return

	def runAnalyseTrain(self, corpus, typeCorpus, directory, fileRes) :
		
		fileAnalyse = self._prepareAnalyse(corpus, typeCorpus, fileRes)
		fileReadyAnalyse = os.path.join(self.dirResult, 'treeTaggerAnalyse.txt')
		
		dependencyDir = os.path.join(self.rootDir, 'dependencies')
		#-threshold 0.5
		command = dependencyDir+"/tree-tagger-linux-3.2/bin/tree-tagger -token -no-unknown "+self.rootDir+"/KB/config/treetagger/french.par "+fileReadyAnalyse+" "+self.dirResult+fileRes
		process = subprocess.Popen(command, shell=True, stdout=subprocess.PIPE)
		process.wait()
		return

	def runAnalyseBeforeTrain(self, fileRes, PosDetails = 0) :

		"""
		reform the training file with result of TreeTagger
		"""

		fanalyseTreeTagger= open(self.dirResult+fileRes, 'r', encoding='utf-8')
		ftrainWapiti= open(self.dirResult+'trainingdata_CRF_Wapiti.txt', 'r', encoding='utf-8')
		fResultFinal= open(self.dirResult+'trainingdata_CRF_TreeTagger_Wapiti.txt', 'w', encoding='utf-8')
 
		myLongList = list(ftrainWapiti)
		myShortList= list(fanalyseTreeTagger)
		taille = len(myLongList)

		j = -1
		for i in range(0, taille):
		    ligne = myLongList[i]
		    mots = ligne.split()

		    nbMots = len(mots)

		    if nbMots == 0 :
		        fResultFinal.write('\n')
		    else :
		        j = j + 1
		        mots2 = myShortList[j].split()
		        if PosDetails == 1:
		            newList = []
		            for w in mots2:
		                a = re.sub("(VER|DET|PRO|PRP|PUN):.+",r'\1', w)
		                newList.append(a)
		                mots2 = newList

		        newLine = "{0} {1}".format(' '.join(mots2), ' '.join(mots[1:nbMots]))
			fResultFinal.write(newLine)
			fResultFinal.write('\n')

		fResultFinal.close()
		fanalyseTreeTagger.close()
		ftrainWapiti.close()
		return

	def runAnalyseBeforeTest(self, fileRes, PosDetails = 0) :
		"""
		reform the test file with result of TreeTagger
		"""

		fanalyseTreeTagger= open(self.dirResult+fileRes, 'r', encoding='utf-8')
		ftestWapiti= open(self.dirResult+'testdata_CRF_Wapiti.txt', 'r', encoding='utf-8')
		fResultFinal= open(self.dirResult+'testdata_CRF_TreeTagger_Wapiti.txt', 'w', encoding='utf-8')

 		myLongList = list(ftestWapiti)
		myShortList= list(fanalyseTreeTagger)
		taille = len(myLongList)

		j = -1
		for i in range(0, taille):
		    ligne = myLongList[i]
		    mots = ligne.split()

		    nbMots = len(mots)

		    if nbMots == 0 :
		        fResultFinal.write('\n')
		        
		    else :
		        j = j + 1
		        mots2 = myShortList[j].split()
		        if PosDetails == 1:
		            newList = []
		            for w in mots2:
		                a = re.sub("(VER|DET|PRO|PRP|PUN):.+",r'\1', w)
		                newList.append(a)
		                mots2 = newList

		        newLine = "{0} {1}".format(' '.join(mots2), ' '.join(mots[1:nbMots]))
			fResultFinal.write(newLine)
			fResultFinal.write('\n')

		fResultFinal.close()
		fanalyseTreeTagger.close()
		ftestWapiti.close()
		return

	def runAnalyseTest(self, corpus, typeCorpus, fileRes) :
		"""
		Analyse of TreeTagger
		"""
		
		fileAnalyse = self._prepareAnalyse(corpus, typeCorpus, fileRes)
		fileReadyAnalyse = os.path.join(self.dirResult, 'treeTaggerAnalyse.txt')
		
		dependencyDir = os.path.join(self.rootDir, 'dependencies')

		command = dependencyDir+"/tree-tagger-linux-3.2/bin/tree-tagger -token -no-unknown "+self.rootDir+"/KB/config/treetagger/french.par "+fileReadyAnalyse+" "+self.dirResult+fileRes
		process = subprocess.Popen(command, shell=True, stdout=subprocess.PIPE)
		process.wait()
		return

	def _retrieveTestContext(self, fileTreeWapiti, Context=0, PosDetails = 0):
		'''
		PosDetails : 0 = POS with detail ex: VER:inf  
		             1 = harmonisation POS ex : VER:inf --> VER
				 
		Context :    0 = Token + POS
		             1 = Only POS
		'''
		
		inputt = open(self.dirResult+fileTreeWapiti, 'r', encoding='utf-8')
		output = open(self.dirResult+'testdata_CRF_TreeTagger_Wapiti_Context.txt', 'w', encoding='utf-8')
		
		inputList = list(inputt)
		taille = len(inputList)
    
		for i in range(0, taille):
		    
			ligne = inputList[i]
			mots = ligne.split()
			currentToken = mots[0:2]
			if PosDetails == 1 :
			    newList = []
			    for w in currentToken:
				a = re.sub("(VER|DET|PRO|PRP|PUN):.+",r'\1', w)
				newList.append(a)
				currentToken = newList
		
			nbMots = len(mots)
			endLine = mots[3:nbMots]
		
		    
			if nbMots == 0 :
			    output.write('\n')
			    continue
		    
			prevline = inputList[i-1] if i !=0  else ""
			previousLine = prevline.split()
		    
			nextline = inputList[i+1] if i != (taille-1) else ""
			nextli = nextline.split()
		
			if Context == 0:
			    nextToken = nextli[0:2]#Token + POS
			    previousWord = previousLine[0:2]
			    
			    if PosDetails == 1 :
				nextTokenList = []
				previousWordList = []
				for nextw in nextToken:
				    a = re.sub("(VER|DET|PRO|PRP|PUN):.+",r'\1', nextw)
				    nextTokenList.append(a)
				    nextToken = nextTokenList
				    
				for prevw in previousWord:
				    a = re.sub("(VER|DET|PRO|PRP|PUN):.+",r'\1', prevw)
				    previousWordList.append(a)
				    previousWord = previousWordList
				
			    if len(previousWord) == 0:
				newLine = "{0} {1} {2} {3}".format(' '.join(currentToken),''.join('NULL NULL'),' '.join(nextToken),' '.join(endLine))
				output.write(newLine)
				output.write('\n')
			    
			    elif len(nextToken) == 0:
				newLine = "{0} {1} {2} {3}".format(' '.join(currentToken),' '.join(previousWord),''.join('NULL NULL'),' '.join(endLine))
				output.write(newLine)
				output.write('\n')
		      
			    else:
		    
				newLine = "{0} {1} {2} {3}".format(' '.join(currentToken),' '.join(previousWord),' '.join(nextToken),' '.join(endLine))
				output.write(newLine)
				output.write('\n')
			else:
			    nextToken = nextli[1:2]#Only POS
			    previousWord = previousLine[1:2]
			    
			    if PosDetails == 1 :
				nextTokenList = []
				previousWordList = []
				for nextw in nextToken:
				    a = re.sub("(VER|DET|PRO|PRP|PUN):.+",r'\1', nextw)
				    nextTokenList.append(a)
				    nextToken = nextTokenList
				
				for prevw in previousWord:
				    b = re.sub("(VER|DET|PRO|PRP|PUN):.+",r'\1', prevw)
				    previousWordList.append(b)
				    previousWord = previousWordList
			    
			    if len(previousWord) == 0:
				newLine = "{0} {1} {2} {3}".format(' '.join(currentToken),''.join('NULL'),' '.join(nextToken),' '.join(endLine))
				output.write(newLine)
				output.write('\n')
				
			    elif len(nextToken) == 0:
				newLine = "{0} {1} {2} {3}".format(' '.join(currentToken),' '.join(previousWord),''.join('NULL'),' '.join(endLine))
				output.write(newLine)
				output.write('\n')
			  
			    else:
		
				newLine = "{0} {1} {2} {3}".format(' '.join(currentToken),' '.join(previousWord),' '.join(nextToken),' '.join(endLine))
				output.write(newLine)
				output.write('\n')

		inputt.close()
		output.close()
		return

	def _prepareTestFileEvaluation(self, fileTree):
		"""

		Retrieve token for Tree Tagger Analyse

		"""
		fanalyseTreeTagger= open(self.dirResult+fileTree, 'r', encoding='utf-8')
		ftestWapiti= open(self.dirResult+'testdata_CRF.txt', 'r', encoding='utf-8')
		fResultFinal= open(self.dirResult+'testdata_CRF_TreeTagger.txt', 'w', encoding='utf-8')

		myLongList = list(ftestWapiti)
		myShortList= list(fanalyseTreeTagger)
		taille = len(myLongList)

		j = -1
		for i in range(0, taille):
		    ligne = myLongList[i]
		    mots = ligne.split()

		    nbMots = len(mots)

		    if nbMots == 0 :
			fResultFinal.write('\n')
		    else :
			j = j + 1
			mots2 = myShortList[j].split()
			newLine = "{0} {1}".format(' '.join(mots2), ' '.join(mots[1:nbMots]))
			fResultFinal.write(newLine)
			fResultFinal.write('\n')

		fResultFinal.close()
		fanalyseTreeTagger.close()
		ftestWapiti.close()
		return

	def _retrieveTrainContext(self, fileTreeWapiti, Context=0, PosDetails = 0):
		
		inputt = open(self.dirResult+fileTreeWapiti, 'r', encoding='utf-8')
		output = open(self.dirResult+'trainingdata_CRF_TreeTagger_Wapiti_Context.txt', 'w', encoding='utf-8')

		inputList = list(inputt)
		taille = len(inputList)
    
		for i in range(0, taille):
		    
			ligne = inputList[i]
			mots = ligne.split()
			currentToken = mots[0:2]
			if PosDetails == 1 :
			    newList = []
			    for w in currentToken:
				a = re.sub("(VER|DET|PRO|PRP|PUN):.+",r'\1', w)
				newList.append(a)
				currentToken = newList
		
			nbMots = len(mots)
			endLine = mots[3:nbMots]
		
		    
			if nbMots == 0 :
			    output.write('\n')
			    continue
		    
			prevline = inputList[i-1] if i !=0  else ""
			previousLine = prevline.split()
		    
			nextline = inputList[i+1] if i != (taille-1) else ""
			nextli = nextline.split()
		
			if Context == 0:
			    nextToken = nextli[0:2]#Token + POS
			    previousWord = previousLine[0:2]
			    
			    if PosDetails == 1 :
				nextTokenList = []
				previousWordList = []
				for nextw in nextToken:
				    a = re.sub("(VER|DET|PRO|PRP|PUN):.+",r'\1', nextw)
				    nextTokenList.append(a)
				    nextToken = nextTokenList
				    
				for prevw in previousWord:
				    a = re.sub("(VER|DET|PRO|PRP|PUN):.+",r'\1', prevw)
				    previousWordList.append(a)
				    previousWord = previousWordList
				
			    if len(previousWord) == 0:
				newLine = "{0} {1} {2} {3}".format(' '.join(currentToken),''.join('NULL NULL'),' '.join(nextToken),' '.join(endLine))
				output.write(newLine)
				output.write('\n')
			    
			    elif len(nextToken) == 0:
				newLine = "{0} {1} {2} {3}".format(' '.join(currentToken),' '.join(previousWord),''.join('NULL NULL'),' '.join(endLine))
				output.write(newLine)
				output.write('\n')
		      
			    else:
		    
				newLine = "{0} {1} {2} {3}".format(' '.join(currentToken),' '.join(previousWord),' '.join(nextToken),' '.join(endLine))
				output.write(newLine)
				output.write('\n')
			else:
			    nextToken = nextli[1:2]#Only POS
			    previousWord = previousLine[1:2]
			    
			    if PosDetails == 1 :
				nextTokenList = []
				previousWordList = []
				for nextw in nextToken:
				    a = re.sub("(VER|DET|PRO|PRP|PUN):.+",r'\1', nextw)
				    nextTokenList.append(a)
				    nextToken = nextTokenList
				
				for prevw in previousWord:
				    b = re.sub("(VER|DET|PRO|PRP|PUN):.+",r'\1', prevw)
				    previousWordList.append(b)
				    previousWord = previousWordList
			    
			    if len(previousWord) == 0:
				newLine = "{0} {1} {2} {3}".format(' '.join(currentToken),''.join('NULL'),' '.join(nextToken),' '.join(endLine))
				output.write(newLine)
				output.write('\n')
				
			    elif len(nextToken) == 0:
				newLine = "{0} {1} {2} {3}".format(' '.join(currentToken),' '.join(previousWord),''.join('NULL'),' '.join(endLine))
				output.write(newLine)
				output.write('\n')
			  
			    else:
		
				newLine = "{0} {1} {2} {3}".format(' '.join(currentToken),' '.join(previousWord),' '.join(nextToken),' '.join(endLine))
				output.write(newLine)
				output.write('\n')

		inputt.close()
		output.close()
		return

	def _prepareTrainFileEvaluation(self, fileTree):
		"""

		Retrieve token for Tree Tagger Analyse

		"""
		fanalyseTreeTagger= open(self.dirResult+fileTree, 'r', encoding='utf-8')
		ftestWapiti= open(self.dirResult+'trainingdata_CRF.txt', 'r', encoding='utf-8')
		fResultFinal= open(self.dirResult+'trainingdata_CRF_TreeTagger.txt', 'w', encoding='utf-8')

		myLongList = list(ftestWapiti)
		myShortList= list(fanalyseTreeTagger)
		taille = len(myLongList)

		j = -1
		for i in range(0, taille):
		    ligne = myLongList[i]
		    mots = ligne.split()

		    nbMots = len(mots)

		    if nbMots == 0 :
			fResultFinal.write('\n')
		    else :
			j = j + 1
			mots2 = myShortList[j].split()
			newLine = "{0} {1}".format(' '.join(mots2), ' '.join(mots[1:nbMots]))
			fResultFinal.write(newLine)
			fResultFinal.write('\n')

		fResultFinal.close()
		fanalyseTreeTagger.close()
		ftestWapiti.close()
		return
